#import "@local/mytemplate:1.0.0": *
#import "@preview/physica:0.9.2": *

= 离散型随机变量

== 随机变量

=== 随机变量的概念

设随机试验的样本空间为$Ω$，如果对于每一个可能的试验结果（样本点） $omega in Ω$，都唯一地存在一个实数值$X(omega)$与之对应，则称$X(omega)$为一个*随机变量*，简记为$X$。

随机变量的概念是对概率空间概念的进一步发展。它将随机实验的结果数量化，从而可以用简洁的数学语言描述繁杂的随机问题，并而提高处理相关问题的效能和效率。随机变量的引入使概率论的研究前进了一大步。

随机变量是一个函数，它的定义域是样本空间，值域是实数集。它的值是随机的，但是它的取值是有规律的。随机变量是对随机现象的数量化描述。

设$X$是一个离散型随机变量，它可能取到的值为$x_1, x_2, dots $（有限个或可数个），称
$
p_k = P({omega in Ω | X(omega) = x_k}) = P(X = x_k), k = 1, 2, dots
$
为$X$的*概率分布律*，简称为*分布律*。

分布律满足下列性质：
- $p_k >= 0, k = 1, 2, dots$
- $sum p_k = 1$

=== 常见的离散型随机变量

==== 两点分布（Bernoulli分布）

设随机变量$X$的分布律为
$
P(X = 0) = p, P(X = 1) = 1 - p
$
其中$0 < p < 1$，则称$X$服从参数为$p$的*两点分布*，记为$X ~ B(1, p)$。

Bernoulli试验：只有两种可能结果的随机试验称为*Bernoulli试验*。

==== 二项分布

二项分布的概率分布律为
$
P(X = k) = binom(n, k) p^k (1 - p)^(n - k), k = 0, 1, dots, n
$
其中$0 < p < 1$，则称$X$服从参数为$n, p$的*二项分布*，记为$X ~ B(n, p)$。

$n$重Bernoulli试验中事件$A$发生的次数为$X$，则$X$服从二项分布。

直接计算$P(X = k)$非常复杂，下面考虑一种极端情况：

当$n$充分大，$p$充分小时二项分布的近似公式。

*Poisson定理*：设$lambda > 0$为给定常数，$n$为任意正整数，$p_n = lambda / n$，则对任意固定的非负整数$k$，有
$
lim_(n -> ∞) binom(n, k) p_n^k (1 - p_n)^(n - k) = (e^(-lambda) lambda^k )/ k!
$

注：这里有一个重要假定，即$n times p_n = λ$为常数。它可以理解成：一段时间内的独立重复试验可以发生无穷次，但出现某一特定结果的平均次数是确定的。上面近似公式的右端项，给出了非常重要的泊松分布！

==== 泊松分布

设随机变量$X$的分布律为
$
P(X = k) = (e^(-lambda) lambda^k) / k!, k = 0, 1, 2, dots
$
其中$lambda > 0$，则称$X$服从参数为$lambda$的*泊松分布*，记为$X ~ "Po"(lambda)$。

根据Poisson定理，Poisson分布可以描绘成大量试验$n -> oo$中稀有事件$p -> 0$出现$k$次的概率分布的数学模型。由Poisson分布引起的Poisson信号流是随机过程的一个重要分支。

设$X ~ "Po"(lambda)$，使得$P(X = k)$取到最大值的$k$称作$X$的*最大可能出现次数*。
- 当$lambda$为正整数时，$P(X = lambda) = P(X = lambda - 1)$取到最大值，此时$X$的最大可能出现次数为$lambda$和$lambda - 1$。
- 当$lambda$为非整数时，$P(X = [lambda])$取到最大值，此时$X$的最大可能出现次数为$[lambda]$。

==== 几何分布

设随机变量$X$的分布律为
$
P(X = k) = (1 - p)^(k - 1) p, k = 1, 2, dots
$
其中$0 < p < 1$，则称$X$服从参数为$p$的*几何分布*，记为$X ~ "Ge"(p)$。

几何分布描述的是在Bernoulli试验中，第一次成功所需的试验次数。相当于连续独立的投掷一面不规则硬币，直到出现正面时停止。

上例中，如果我们已经投掷n次，且始终背面朝上，则有
$
P(X = k + n | X > n) = P(X = k), k = 1, 2, dots
$
这是几何分布一个极其特殊的性质：*无记忆性*。它表示未来发生的事情与过去无关。

*定理：*设$X$为取正整数值的随机变量，则下列命题等价：
- $X$服从几何分布
- $P(X > k + n | X > n) = P(X > k), k = 0, 1, 2, dots$
- $P(X = k + n | X > n) = P(X = k), k = 1, 2, dots$
上面的定理不仅说明了*几何分布具有无记忆性*；也说明这一性质是几何分布所特有的。
_证明：_
- $(1) => (2)$：$P(X > k + n | X > n) = P(X > k + n, X > n) / P(X > n) = q^k = P(X > k)$
- $(2) => (3)$：_显然_
- $(3) => (1)$：_递推即可_

==== 超几何分布

设随机变量$X$的分布律为
$
P(X = k) = (binom(m, k) binom(n - m, s - k)) / binom(n, s), k = max(0, s - n + m), ...,min(m, s)
$
其中$0 <= m <= n, 0 <= s <= n$，则称$X$服从参数为$n, m, s$的*超几何分布*，记为$X ~ "Sg"(n, m, s)$。

*二项分布可以看做是有放回的抽样，而超几何分布则可看成是无放回的抽样。*

$n -> oo$时，$m/n -> p$，则从中抽取$s$个元素，其中$m$个元素的概率分布趋近于二项分布：
$
(binom(m, k) binom(n - m, s - k)) / binom(n, s) -> binom(s, k) p^k (1 - p)^(s - k)
$

== 数学期望和方差

=== 数学期望

设离散型随机变量$X$的分布律为
$
P(X = x_k) = p_k, k = 1, 2, dots
$
若级数
$
E(X) = sum x_k p_k
$
绝对收敛#footnote[定义中的绝对收敛是对X可以取到无穷可数个值而言的，
它保证级数的收敛性与项的顺序无关。]，则称级数$E(X)$收敛，称$E(X)$为$X$的*数学期望*，简称*期望*。数学期望就是（加权）平均值，在统计学中也称为均值。

==== 常见离散型随机变量的数学期望

*两点分布的期望：*
$
E(X) = 1 times p + 0 times (1 - p) = p
$

#newpara()
*二项分布的期望：*
$
E(X) = sum_(k = 0)^n k binom(n, k) p^k (1 - p)^(n - k) = sum_(k = 1)^n n binom(n - 1, k - 1) p^k (1 - p)^(n - k) =^("显然") n p
$

#newpara()
*泊松分布的期望：*
$
E(X) = sum_(k = 0)^∞ k (e^(-lambda) lambda^k) / k! = lambda e^(-lambda) sum_(k = 1)^∞ lambda^(k - 1) / (k - 1)! = lambda e^(-lambda) e^lambda = lambda
$

#newpara()
*几何分布的期望：*
$
E(X) &= sum_(k = 1)^∞ k (1 - p)^(k - 1) p = p (1-p)^(-1) sum_(k = 1)^∞ sum_(l = 1)^(k) q^k \
&= p (1 - p)^(-1) sum_(l = 1)^∞ sum_(k = l)^∞ q^k \
&= 1 / p
$

#newpara()
*超几何分布的期望：*
$
E(X) = sum_(k = max(0, s - n + m))^min(m, s) k (binom(m, k) binom(n - m, s - k)) / binom(n, s) = (s m) / n sum_(k = 1)^min(m, s) (binom(m-1, k-1) binom(n - m, s - k)) / binom(n-1, s-1) = (s m) / n
$

#newpara()

==== 与函数复合的数学期望

设离散型随机变量$X$的分布律为$P(X = x_k) = p_k; k = 1, 2, ... $，考虑函数$phi : {x_1, x_2, · · · } -> RR$，即它是定义域包含$X$的取值范围的一个实函数。因此$Y = φ(X)$也是一个随机变量，它的分布律由$X$的分布律完全确定
$
P({omega in Omega | phi(X(omega)) = y}) = sum_(k: phi(x_k) = y) p_k, y in {phi(x_1), phi(x_2), · · · }
$

若$sum_k phi(x_k) p_k$绝对收敛，则称$E(φ(X)) = sum_k phi(x_k) p_k$为$φ(X)$的数学期望。

==== 矩

$n$阶原点矩和$n$阶中心矩：
$
E(X^n) &= sum x_k^n p_k\
E(X - E(X))^n &= sum (x_k - E(X))^n p_k
$

#newpara()
有Vandermode等式：
$
mat(
  1, 1, 1, dots, 1 ;
  x_1, x_2, x_3, dots, x_n ;
  x_1^2, x_2^2, x_3^2, dots, x_n^2 ;
  dots ;
  x_1^(n - 1), x_2^(n - 1), x_3^(n - 1), dots, x_n^(n - 1)
)
mat(
  p_1;
  p_2;
  p_3;
  dots;
  p_n
)
= 
mat(
  1;
  E(X);
  E(X^2);
  dots;
  E(X^(n - 1))
)
$
事实上前$n$阶矩和概率分布律是一一对应的。

=== 数学期望的性质

设$X, Y$是两个随机变量，若$E(X), E(Y)$存在，则有
- $|E(X)| <= E(|X|)$
- 对实数$a < b$，若$a <= X <= b$，则$a <= E(X) <= b$
- 若$X <= Y$，则$E(X) <= E(Y)$
- 对实数$a, b$，有$E(a X + b Y) = a E(X) + b E(Y)$
- 若$X,Y$相互独立，则$E(X Y) = E(X) E(Y)$
- 【Schwarz不等式】$|E(X Y)|^2 <= E(X^2) E(Y^2)$
这些证明都是显然的，只需要用定义写出即可。

这就意味着：可以将随机变量$X$分解成若干个随机变量（这些随机变量是否独立和相关，并不重要）之和，通过求这若干个随机变量的期望而求得$X$的期望。

这样我们可以重新计算二项分布的期望：
$
E(X) = E(sum_(i = 1)^n X_i) = sum_(i = 1)^n E(X_i) = n p
$

事实上，对于样本空间Ω中的随机事件A，我们可以定义相应的*示性函数*$I_A: Ω -> {0, 1}$：
$
I_A(omega) = cases(
  1 quad &omega in A ,
  0 quad &omega in.not A
)
$
则$I_A$是一个随机变量，且$E(I_A) = P(A)$。这意味着：*事件的概率都可以写成它的示性函数的数学期望*。换句话说，数学期望是概率的一种推广。

=== 方差及其性质

考虑离散型随机变量$X$，若$E((X - E(X))^2)$存在，则称$E((X - E(X))^2)$为$X$的*方差*，记为$D(X)$或$"Var"(X)$：
$
D(X) = E((X - E(X))^2) = E(X^2) - (E(X))^2
$
方差刻画了$X$的分散程度，因为X越分散，用一个实数去代表它的误差就越大。

==== 方差的性质

*平均平方误差*：任取$a in RR$，我们有
$
E((X - a)^2) &= E(((X - E(X)) + (E(X) - a))^2) \
&= E((X - E(X))^2) + 2(E(X) - a) E(X - E(X)) + (E(X) - a)^2 \
&= D(X) + (E(X) - a)^2 >= D(X)
$
上式表明，若用一个实数去表示一个随机变量$X$， 数学期望$E(X)$表示使平均平方误差最小的实数，而最小的平均平方误差就是方差$D(X)$。

*两个随机变量的方差*：考虑离散型随机变量$X,Y$，若$D(X), D(Y)$存在，则有
- 对实数$a, b$，有$D(a X + b) = a^2 D(X)$
- 若$X,Y$相互独立，则$D(X + Y) = D(X) + D(Y)$
- $D(X) = 0$当且仅当$P(X = E(X)) = 1$，即$X$几乎必然是一个常数
前两个证明显然，第三个的必要性考虑一定有正部分即可。

==== 常见离散型随机变量的方差

*二项分布的方差：*
$
D(X) = E(X^2) - (E(X))^2 = n p (1 - p)
$

#newpara()

*泊松分布的方差：*
$
D(X) = E(X^2) - (E(X))^2 = lambda
$

==== Chebyshev不等式

*Chebyshev不等式*：设$X$是一个随机变量，$E(X)$和$D(X)$存在，则对任意实数$epsilon > 0$，有
$
P(|X - E(X)| >= epsilon) <= D(X) / epsilon^2
$
_证明：_
记离散型随机变量$X$可能取到的值为${x_i}$他们相应的概率为${p_i}$。为简化，我们记
$
Lambda = {x_i}, macron(x) = E(X), Gamma = { i | x_i in Lambda, |x_i - macron(x)| >= epsilon }
$
从而
$
P(|X - E(X)| >= epsilon) = sum_(i in Gamma) p_i <= sum_(i in Gamma) p_i (x_i - macron(x))^2 / epsilon^2 <= sum_(i in Lambda) p_i (x_i - macron(x))^2 / epsilon^2 = D(X) / epsilon^2
$
Chebyshev不等式说明了一个随机变量$X$远离期望$E(X)$的概率被方差$D(X)$的大小和距离$epsilon$的大小所控制。这是一个比较松的不等式。

== 条件分布

=== 随机变量的条件分布

给定样本空间$Ω$，考虑定义在其上的离散型随机变量$X,Y$，则称$vec(X,Y)$为*2维离散型随机向量*。进一步的，若$vec(X,Y)$可能的取值为$vec(x_i,y_j)$，则称
$
p_(i j) = P(X = x_i, Y = y_j)
$
为$vec(X,Y)$的*联合分布律*，简称分布律。我们称
$
P(X = x_i) = sum_(j) p_(i j), P(Y = y_j) = sum_(i) p_(i j)
$
为$X,Y$的*边缘分布律*。进一步的，我们可以定义$X$关于$Y = y_j$的*条件分布律*为
$
P(X = x_i | Y = y_j) = P(X = x_i | Y = y_j)  / P(Y = y_j)
$
这里$P(Y = y_j) > 0$。这个条件分布律的物理意义是：在给定$Y = y_j$的条件下，$X$的取值概率。而$Y$关于$X = x_i$的条件分布律为
$
P(Y = y_j | X = x_i) = P(X = x_i | Y = y_j)  / P(X = x_i)
$

=== Markov链

考虑离散型随机变量序列${X_n}_(n = 1)^oo, X_n in S$，其中$S$是一个有限或者可数集（称为此*Markov链的状态空间*）。若对任意$i_1, ...,i_n,i,j$，有
$
P(X_(n + 1) = j | X_1 = i_1, ..., X_(n-1) = i_(n-1), X_n = i) = P(X_(n + 1) = j | X_n = i)
$
则称其为*Markov序列(或Markov链)*，其条件概率
$
hat(p)_(i j) (n)= P(X_(n + 1) = j | X_n = i)
$
称为*转移概率*。若$hat(p)_(i j) (n)$与$n$无关，称此Markov链为*时间齐次*的。

*Markov链的性质是：未来的状态只与当前状态有关，而与过去的状态无关。* 如果过程在$n$时刻处于状态$i$，那么不管它以前处于什么状态，它在$n + 1$时刻处于状态$j$的条件概率都是$hat(p)_(i j) (n)$。换句话说，在已知“现在”的条件下，“将来”与“过去”无关。

对于时间齐次的Markov链，我们可以将转移概率写成矩阵形式
$
PP = mat(
  hat(p)_(1 1), hat(p)_(1 2), dots, hat(p)_(1 s) ;
  hat(p)_(2 1), hat(p)_(2 2), dots, hat(p)_(2 s) ;
  dots ;
  hat(p)_(s 1), hat(p)_(s 2), dots, hat(p)_(s s)
)
$
称为*转移概率矩阵*，是一个$|S| times |S|$的矩阵。

_例如：将一枚均匀硬币投掷无穷次，记$X_n$为前$n$次投掷中正面出现的次数，易见_
$
P(X_(n + 1) = j | X_n = i) = cases(
  1/2 quad &j = i + 1 ,
  1/2 quad &j = i ,
  0 quad &"others"
)
$
_这就是一个Markov链。对应的转移概率矩阵为_
$
PP = mat(
  1/2, 1/2, 0, , ;
  0, 1/2, 1/2, 0 ,;
  "", 0, 1/2, 1/2, 0 ;
  "", "", dots.down, dots.down, dots.down ,dots.down;
)
$

_Google搜索引擎的核心技术是通过PageRank对多达30亿个网页进行重要性分析。PageRank利用网络链接结构对网页进行组织管理，原理是，如果网页A链接到网页B，Google就认为“网页A投网页B一票”。_

_考虑Google数据库中网页全体组成的集合为$S$（状态空间） ，其元素个数为$n ∼ 10^10$。定义一个$n$阶矩阵$G$：如果从页面$i$到页面$j$有超链接，则$g_(i j) = 1$，否则为$0$。易见$G$是巨大的且非常稀疏的矩阵。令_
$
c_j = sum_i g_(i j), r_i = sum_j g_(i j)
$
_分别给出每个页面链入和链出的数目。由此给出Markov链的转移概率矩阵$PP = (hat(p)_(i j))$：第一项表示用户链接到其它网页是等可能的（其概率为$0.85/r_i$）；第二项表示用户有可能不选择网页提供的链接方案（其概率为$0.15/n$）_
$
hat(p)_(i j) = tau g_(i j) / r_i + delta, delta = (1 - tau)/n
$
_其中$PP$的最大特征值为1，对应的特征向量即为PageRank_
$
pi PP = pi, sum_i pi_i = 1
$
_这里$π$就是*Markov链的平稳分布*。因此，计算PageRank的问题，本质上是大规模矩阵特征值问题。_

=== 随机变量的独立性

称离散型随机变量$X$和$Y$是相互独立的，如果
$
P(X = x_i, Y = y_j) = P(X = x_i) P(Y = y_j) , i,j = 1, 2, dots
$
由定义，若$X,Y$相互独立，则有
$
P(X = x_i | Y = y_j) = P(X = x_i)\
P(Y = y_j | X = x_i) = P(Y = y_j)
$
*一个随机向量的联合分布唯一的确定了作为其分量的随机变量的边缘分布。*但是，仅有边缘分布的信息对于确定他们的联合分布是不够的。*只有在独立性假设下，才可以由边缘分布唯一确定其联合分布。*

若离散型随机变量$X,Y$相互独立，且$E(X),E(Y)$存在，则$E(X Y) = E(X)E(Y)$。若$X,Y$相互独立，且$D(X),D(Y)$存在，则$D(X + Y) = D(X) + D(Y)$。

_证明：_
$
E(X Y) = sum_(i,j) x_i y_j p_(i j) = sum_(i,j) x_i y_j p_i p_j = sum_i x_i p_i sum_j y_j p_j = E(X) E(Y)
$
$
D(X + Y) &= E((X + Y) - (E(X + Y))^2) = E(((X - E(X)) + (Y - E(Y)))^2) \
&= E((X - E(X))^2) + 2 E((X - E(X))(Y - E(Y))) + E((Y - E(Y))^2) \
&= #text(fill:red)[$D(X) + D(Y) + 2 E((X - E(X))(Y - E(Y)))$] \
&= D(X) + D(Y) + 2 E(X Y) - 2 E(X) E(Y) = D(X) + D(Y)
$

#newpara()
如果$X$和$Y$是相互独立的整数值随机变量，其分布为
$
P(X = i) = p_i, P(Y = j) = q_j; i,j = 0 plus.minus 1, plus.minus 2, dots
$
则$X + Y$的分布为
$
P(X + Y = k) = sum_(i = -oo)^oo p_i q_(k - i)
$
此分布称为$X$的分布律与$Y$的分布律的*卷积*。

设$X_i$为离散型随机变量，如果对于$X_k$任意的可能取值$x_k$，都有
$
P(X_1 = x_1, X_2 = x_2, dots, X_n = x_n) = P(X_1 = x_1) P(X_2 = x_2) dots P(X_n = x_n)
$
则称$X_1, X_2, dots, X_n$是*相互独立*的。我们称随机变量序列${X_n, n>=1}$为相互独立的，如果对任意的$n$有$X_1, X_2, ... , X_n$相互独立。

一些性质：

- 相互独立的随机变量$X_1, X_2, ... , X_n$的任一部分随机变量$X_(j_1), X_(j_2), ... , X_(j_k)$也是相互独立的。
- 若$X_1, X_2, dots, X_n$相互独立，而
  $
  Y_1 = g_1(X_1, X_2, dots, X_k), Y_2 = g_2(X_(k+1), X_(k+2), dots, X_n)
  $
  则$Y_1, Y_2$也是相互独立的。

若离散型随机变量$X, Y$的取值均为${a_i}$，且对任意的$i$
$
P(X = a_i) = P(Y = a_i)
$
则称$X$和$Y$是*同分布*的。

如果${X_n, n ≥ 1}$是一个独立的随机变量序列，并且他们是同分布的，则称${X_n, n ≥ 1}$是一个*独立同分布的随机变量序列*，简记为i.i.d(independent and identically distributed)。

=== 协方差和相关系数

设$X,Y$是两个随机变量，有
$
D(X + Y) = D(X) + D(Y) + 2 E((X - E(X))(Y - E(Y)))
$
如果$X, Y$独立，则最后一项为$0$； 如果$X, Y$不独立，则最后一项在一定程度上刻画了$X, Y$的相关性。

设$vec(X,Y)$是一个二维随机向量，若$E((X - E(X))(Y - E(Y)))$存在，则称
$
"Cov"(X,Y) = E((X - E(X))(Y - E(Y)))
$
为$X,Y$的*协方差*。若$D(X)D(Y)!=0$，则称
$
rho_(X Y) = "Cov"(X,Y) / sqrt(D(X) D(Y))
$
为$X,Y$的*相关系数*。

有
$
"Cov"(X,X) = D(X), rho_(X X) = 1
$

#newpara()

*协方差具有如下性质：*
- $"Cov"(X,Y) = E(X Y) - E(X) E(Y)$
- $"Cov"(X,Y) = "Cov"(Y,X)$
- $"Cov"(a_1 X_1 + b_1, a_2 X_2 + b_2) = a_1 a_2 "Cov"(X_1,X_2)$
- $"Cov"(a_1 X_1 + a_2 X_2, Y) = a_1 "Cov"(X_1,Y) + a_2 "Cov"(X_2,Y)$
- $|"Cov"(X,Y)|^2 <=D(X) D(Y)$【Schwarz不等式】
- 若$X,Y$相互独立，则$"Cov"(X,Y) = 0$

*相关系数具有如下性质：*
- $-1 <= rho_(X Y) <= 1$
- $rho_(X Y) = 1$当且仅当$X$和$Y$以概率为$1$线性相关，即
  $
  P(Y = a X + b) = 1
  $
  其中$a,b$是常数，且$a != 0$
当$a_(X Y) != 0$时，根据其符号是正或负，分别称$X, Y$具有*正相关或负相关*。

*用随机变量逼近随机变量：*

方差：$D(X)$， 如果用一个实数$b in RR$去逼近随机变量$X$，数学期望$E(X)$表示使平均平方误差最小的实数，而这个最小的平均平方误差就是方差$D(X)$。
$
E((X - b)^2) >= E((X - E(X))^2) = D(X) 
$
基于上述思路，我们希望引入一个新的随机变量$Y$，用它的线性函数$a Y + b$，去逼近随机变量$X$，使其平均平方误差最小。

下证：
$
E((X - (hat(a) Y + hat(b)))^2) = min_(a,b) E((X - a Y - b)^2) = D(X)(1 - rho_(X Y)^2)
$
其中$hat(a) = rho_(X Y) sqrt(D(X) /D(Y)), hat(b) = E(X) - hat(a) E(Y)$。

_证明：_
$
E((X - a Y - b)^2) &= E(((X - E(X)) - a(Y - E(Y)) + (E(X) - a E(Y) - b))^2) \
&= D(X)+ a^2 D(Y) + (E(X) - a E(Y) - b)^2  - 2 a E((X - E(X))(Y - E(Y))) \
&= D(X) + a^2 D(Y) + (E(X) - a E(Y) - b)^2 - 2 a "Cov"(X,Y) \
&>= D(X) + a^2 D(Y) - 2 a "Cov"(X,Y) \
""^("关于a的二次函数") &>=D(X) - "Cov"(X,Y)^2 / D(Y) = D(X)(1 - rho_(X Y)^2)
$
等号成立的条件为$a = rho_(X Y) sqrt(D(X) /D(Y))$与$b = E(X) - a E(Y)$。

该定理说明相关系数$ρ_(X Y)$刻画了$X, Y$之间的线性相关关系。如果$ρ_(X Y) = 0$，即$hat(a) = 0$，用$Y$去近似$X$的最佳情形是只用常数，而与$Y$无关。此时称$X$与$Y$线性不相关，简称*不相关*。

如果$ρ_(X Y)$越大，用随机变量$Y$的线性函数去近似随机变量X的最佳近似的均方误差越小。 当 $ρ_(X Y) = 1$时，以概率1，随机变量$X$是随机变量$Y$的线性函数。上述性质正是将$ρ_(X Y)$称为$X$与$Y$的*线性相关系数*的原因，即相关系数是刻画$X$和$Y$的线性相关程度的数字特征。

*独立一定不相关，但不相关不一定独立。*独立性是更强的条件。

如果随机变量$X$是有度量的单位，例如米，那么方差$D(X)$的单位就是平方米，而标准差$σ_X = sqrt(D(X))$与$X$具有同样的度量单位米。为了使随机偏差摆脱度量单位的影响，我们给出随机变量$X$的*标准化*
$
X^* = (X - E(X)) / σ_X
$
这样
$
E(X^*) = 0, D(X^*) = 1, "Cov"(X^*,Y^*) = rho_(X^* Y^*)
$

#newpara()

*一个恒等式：*设${X_i}$为$n$维随机向量，则
$
D(sum_(i = 1)^n X_i) = sum_(i = 1)^n D(X_i) + 2 sum_(i < j) "Cov"(X_i, X_j)
$
证明是显然的。

设$X,Y$是随机变量，则有：
- 若$E(X^n)$存在，则称为$X$的$n$阶*原点矩*
- 若$E((X - E(X))^n)$存在，则称为$X$的$n$阶*中心矩*
- 若$E(X^n Y^m)$存在，则称为$X,Y$的$n+m$阶*混合矩*
- 若$E((X - E(X))^n (Y - E(Y))^m)$存在，则称为$X,Y$的$n+m$阶*中心混合矩*

期望$E(X),E(Y)$是一阶原点矩，方差$D(X),D(Y)$是二阶中心矩，协方差$"cov"(X, Y)$是二阶中心混合矩。

随机变量可以是无穷维的，但是矩一般只考虑有限个。用有限的信息（矩）去刻画无穷维的随机变量，这是非常有价值的事情。

*协方差矩阵：*设$vec(X)$是一个$n$维随机向量，其分量为$X_1, X_2, ... , X_n$，则称
$
Sigma = mat(
  "Cov"(X_1, X_1), "Cov"(X_1, X_2), dots, "Cov"(X_1, X_n) ;
  "Cov"(X_2, X_1), "Cov"(X_2, X_2), dots, "Cov"(X_2, X_n) ;
  dots ;
  "Cov"(X_n, X_1), "Cov"(X_n, X_2), dots, "Cov"(X_n, X_n)
)
$
为$vec(X)$的*协方差矩阵*。协方差矩阵是一个对称矩阵，且对角线上的元素是各个随机变量的反差，非对角线上的元素是各个随机变量之间的协方差。

有这样的性质：
- $Sigma$是对称的
- $sigma_(i j)^2 <= sigma_(i i) sigma_(j j)$
- $Sigma$是非负定的，即对任意非零向量$bold(a)$，有$bold(a)^T Sigma bold(a) >= 0$

_证明：_
$
bold(a)^T Sigma bold(a) = sum_(i,j) a_i a_j "Cov"(X_i, X_j) = "Cov"(sum_i a_i X_i, sum_j a_j X_j) >= 0
$

=== 分布的熵

Shannon引进熵的概念作为刻画一个离散分布或一个离散随机变量的不确定性的指标。熵是目前信息科学中最重要的概念之一。

离散分布$bold(p)(X = x_i) = p_i, i = 1, 2, ... , n$的熵定义为
$
H(bold(p)) = - sum p_i log p_i
$
再定义分布$bold(p)$关于分布$bold(q)$的*Kullback-Leibler相对熵*为
$
h(bold(p), bold(q)) = sum p_i log (p_i / q_i)
$
这个相对熵刻画了两个分布之间的差异。

相对熵$h(bold(p), bold(q))$是非负的，当且仅当$bold(p) = bold(q)$时取到最小值$0$。这个性质是由Jensen不等式保证的。

相对熵在相当程度上表达了$bold(p)$和$bold(q)$的差别，当$p_i$和$q_i$都接近时，显然$h(bold(p), bold(q))$会小，因此可以看成$bold(p)$和$bold(q)$之间的准距离。#footnote[它不满足对称性，也不满足三角不等式，因此不是数学中的距离。]

*有限个值的分布的熵：*分布$bold(p)$的熵满足
$
H(bold(p)) = - sum p_i log p_i <= log N
$
其中$N$是分布$bold(p)$可能取到的值的个数。等号成立当且仅当$bold(p)^* = (1/N, 1/N, ... , 1/N)$，即所有可能取到的值的概率相等时。 

*数学期望固定条件下的最大熵：*假设存在实数$alpha$，使得$x_i >= alpha$。对于固定的$(x_i)$和$mu$，在满足
$
P(xi = x_i)= p_i, E(X) = mu
$
的条件下，熵$H(bold(p))$的最大的分布为
$
p_i^* = C  e^(-lambda x_i), C = 1 / (sum e^(-lambda x_i))
$
其中$lambda$满足
$
mu sum e^(-lambda x_i) = sum x_i e^(-lambda x_i)
$
这个最大熵是
$
H(bold(p)^*) = lambda mu - log C
$

=== 条件数学期望

设$X,Y$是两个随机变量，定义$E(X|Y=y)$是在$Y=y$的条件下$X$的数学期望，定义为
$
E(X|Y=y) = sum x_i P_(X|Y)(x_i|y)
$
$E(X|Y=y)$可以看作关于$y$的函数。记作$phi(y)$，则随机变量$phi(Y(omega))$是一个随机变量，称为$X$关于$Y$的*条件数学期望*，记作$E(X|Y)$。

$X$对$Y$的条件数学期望$E(X|Y)$是随机变量$Y$的一个函数，因此$E(X|Y)$是随机变量。

*全期望公式：*对随机变量$X,Y$，若$E(X)$存在，并且$P(Y = y_j) > 0$，则有
$
E(X) = E(E(X|Y)) = sum E(X|Y = y_j) P(Y = y_j)
$
_证明：_
$
E(E(X|Y)) &= E(phi(Y)) = sum phi(y_j) P(Y = y_j) \
&= sum E(X|Y = y_j) P(Y = y_j)\
&= sum sum x_i P_(X|Y)(x_i|y_j) P(Y = y_j) \
&= sum sum x_i P(X = x_i, Y = y_j)\
&= sum x_i P(X = x_i) = E(X)
$
由于数学期望可以看作是概率的推广，全期望公式也可以看作是全概率公式的推广。

*平均平方误差估计：*对随机变量$X,Y$，若$E(X^2),E(Y^2)$均存在，则有  
$
E((X - E(X|Y))^2) min_phi E((X - phi(Y))^2)
$
_证明：_
由于$E(X|Y)$是关于随机变量$Y$的函数，不妨记其为$g(Y)$，则有
$
cal(J) = E((X - g(Y))^2) = E(X-g(Y))^2 = E(E((X-g(Y))^2|Y)) 
$
上式最后一步利用了全期望公式。这里的$E((X-g(Y))^2|Y)$是关于$Y$的函数，记作$h(Y)$，则有
$
cal(J) = E(h(Y)) = sum h(y_j) P(Y = y_j) = sum E((X-g(Y))^2|Y = y_j) P(Y = y_j)
$
对于任意函数$phi$有
$
E((X - phi(Y))^2|Y=y_j) =& E((X - g(Y) + g(Y) - phi(Y))^2|Y=y_j) \
=& E((X - g(Y))^2|Y=y_j) + 2 E((X - g(Y))(g(Y) - phi(Y))|Y=y_j) \
 &+ E((g(Y) - phi(Y))^2|Y=y_j) \
=& E((X - g(Y))^2|Y=y_j) + (g(y_j) - phi(y_j))^2 + 2 (g(y_j)\
 &- phi(y_j)) E((X - g(Y))|Y=y_j)\
=& E((X - g(Y))^2|Y=y_j) + (g(y_j) - phi(y_j))^2\
>=& E((X - g(Y))^2|Y=y_j)
$
上式最后一步利用了$E((X - g(Y))|Y=y_j) = 0$。因此
$
cal(J) &= E((X - g(Y))^2) \
&= E(E((X - g(Y))^2|Y)) \
&= sum E((X - g(Y))^2|Y = y_j) P(Y = y_j) \
&<= sum E((X - phi(Y))^2|Y = y_j) P(Y = y_j)\
&= E((X - phi(Y))^2)
$
在用可观测的$Y$去估计$X$时，条件期望$E(X|Y)$是所有估计中最佳的。

*条件期望的性质：*若随机变量$E(X|Y)$存在，则它满足以下性质：
- 若$a <= X <= b$，则$a <= E(X|Y) <= b$
- 若$E(X_1|Y)$和$E(X_2|Y)$均存在，则$E(a_1 X_1 + a_2 X_2|Y) = a_1 E(X_1|Y) + a_2 E(X_2|Y)$
- 若$X$和$Y$相互独立，则$E(X|Y) = E(X)$
- 设$(X,Y)$是一个二维随机向量，$g,h$是两个实函数，则$E(g(X)h(Y)|Y) = h(Y)E(g(X)|Y)$
- 若$Phi$是定义在${y_1, y_2, ... , y_n, ...}$上的函数，则$Phi(Y) = E(X|Y)$当且仅当对任意定义在${y_1, y_2, ... , y_n, ...}$上的函数$phi$，有$E((X - Phi(Y))phi(Y)) = 0$

= 随机徘徊、泊松过程

== 随机徘徊

不停的投掷一枚硬币，其正面朝上的概率为$p$。现有一个粒子，初始位置为0，根据投掷结果在直线上移动：如投正面，粒子向右走1步；如投反面，粒子向左走1步。

根据上述描述，我们可以给出这个试验的样本空间
$
Omega = {bold(omega) = (omega_1, omega_2, ... , omega_n, ...)}
$
我们还可以给出相应的概率空间$(Omega, cal(F), P)$。由此，我们定义随机变量的序列
$
Y_n(bold(omega)) = omega_n\
Z_n(bold(omega)) = 2 Y_n(bold(omega)) - 1\
X_n(bold(omega)) = sum_(i = 1)^n Z_i(bold(omega)) + X_0
$
其中$Y_n$表示第$n$次投掷的结果，$Z_n$表示第$n$次投掷的结果对应的粒子移动的方向，$X_n$表示第$n$次投掷后粒子的位置。

这样的独立同分布随机序列${Z_n,n ≥ 1}$称为Bernoulli序列，它是一个最简单的随机过程。

*状态空间：*$S_z = {-1, 1}，S_x = ZZ$。称每一个固定的$bold(omega) = (omega_1, omega_2, ... , omega_n, ...)$为一个*样本轨道*。

下面导出$X$的有限个随机变量的联合分布律。设$X_0 = 0$，则有
$
P(X_n = s) = cases(
  binom(n, (n + s) / 2) p^((n + s) / 2) (1 - p)^((n - s) / 2) quad &n>=|s| n+s"为偶数",
  0 quad &"others"
)
$

再讨论$(X_n, X_m)$的联合分布，这里$m > n$，则有
$
P(X_n = i, X_m = j) &= P(X_n = i, X_m - X_n = j - i) = P(sum_(i = 1)^n Z_i = i, sum_(i = n+1)^m Z_i = j - i)\
&= P(sum_(i = 1)^n Z_i = i) P(sum_(i = n+1)^m Z_i = j - i)\
&= binom(n, (n + i) / 2) p^((n + i) / 2) (1 - p)^((n - i) / 2) binom(m - n, (m - n + j - i) / 2) p^((m - n + j - i) / 2) (1 - p)^((m - n - j + i) / 2)
$

面是多个时刻$(X_(n_1), X_(n_2), ... , X_(n_k) n_1 < n_2 < ... < n_k$的联合分布，则有
$
P(X_(n_1) = i_1, X_(n_2) = i_2, ... , X_(n_k) = i_k) &= product_(j = 1)^k P(X_(n_j) = i_j | X_(n_(j-1)) = i_(j-1))\
$

可以计算出$X_n$的数学期望和方差
$
E(X_n) = 2 n p - n\
D(X_n) = 4 n p (1 - p)
$

最后计算$(X_n, X_m)$的协方差
$
"Cov"(X_n, X_m) = 2 min(n, m) p (1 - p)
$

#newpara()

*带吸收壁的随机徘徊*： 考虑粒子从0出发，走到$a$或$-b$处则结束$(a, b > 0)$，问粒子走到$a$处或$-b$处的概率分别有多大。

*首次到达问题*：考虑粒子从0出发，记
$
T_(0a) = min{t >= 0 | X_0 = 0,X_t = a}
$
表示首次到达$a$的时刻。如果
$
{t>=0|X_0 = 0, X_t = b} = emptyset
$
则粒子无法到达$b$，记$T_(0b) = +oo$。特别地，$T_(00)$表示首次返回原点的时刻。

=== 随机过程

对于概率空间$(Omega, cal(F), p)$，一簇依赖于参数$t in T$的随机变量$X_t$，称为*随机过程*，记作${X_t, t in T}$。其中$T$是*指标集*，通常表示时间。

随机过程${X_t, t in T}$的*样本轨道*是指对于每一个固定的$omega in Omega$，$t -> X_t(omega)$是一个函数，称为*样本轨道*。

我们常把$t$解释为时间。一般来说，$T$是一个无限集合。如果它是可数集合，如$T = ZZ$，则称$X$为*离散参数的随机过程*或*随机序列*，即
$
X = {X_n, n in ZZ}
$
如果$T = RR$，则称$X$为*连续参数的随机过程*。

#figure(
  tablem[
    | $t$取值\\$X$取值 | 连续 | 离散 |
    | ----------------- | ---- | ---- |
    |连续 | Brown运动 | Poisson过程 |
    |离散 | Markov链 | 随机徘徊 |
  ],
  caption: [
    随机过程的分类
  ],
  kind: table
)

设$X = {X_t : t in T}$是一个随机过程，若它在任意$s$个互不相交的区间
$
[m_i , n_i] sect [m_j , n_j] = emptyset, i != j
$
上的增量$X_(n_i) - X_(m_i)$是相互独立的，则称$X$是*独立增量过程*。

若对于$n>0$，$X_(t + n) - X_t$的分布对一切$t in T$都是相同的，则称$X$是*时齐的独立增量过程*。

一维和二维随机过程都是时齐的独立增量过程。

== 泊松过程

=== 泊松分布的性质

泊松分布：

设$X$是一个随机变量，如果它的概率分布为
$
P(X = k) = (lambda^k e^(-lambda))/k!, k = 0, 1, 2, ...
$
则称$X$服从参数为$lambda$的泊松分布，记作$X ~ P(lambda)$。

泊松分布的期望和方差为
$
E(X) = D(X) = lambda
$

==== 可加性

设$X_1, X_2, ... , X_n$是$n$个独立的服从参数为$lambda_i$的泊松分布的随机变量，则$Y = sum_(i = 1)^n X_i$服从参数为$sum_(i = 1)^n lambda_i$的泊松分布。

_证明：_

由于$X_i$是独立的，我们线计算$n=2$的情况，后面归纳即可。

对于$n=2$，$Y$的分布律为
$
P(Y = k) &= sum_(i = 0)^k P(X_1 = i) P(X_2 = k - i) = sum_(i = 0)^k (lambda_1^i e^(-lambda_1))/i! (lambda_2^(k-i) e^(-lambda_2))/(k-i)!\
&= sum_(i = 0)^k binom(k, i) lambda_1^i lambda_2^(k-i) e^(-lambda_1 - lambda_2)/k! = (lambda_1 + lambda_2)^k e^(-(lambda_1 + lambda_2))/k!\
&= (lambda_1 + lambda_2)^k e^(-(lambda_1 + lambda_2))/k!
$

==== 随机选择下的不变性

从$X ~ P(lambda)$个人中，以保留概率$p$的概率随机选择$Y$个人，则$Y ~ P(lambda p)$。

_证明：_
$
P(Y = k) &= sum_(n=k)^oo P(Y = k | X = n) P(X = n) = sum_(n=k)^oo binom(n, k) p^k (1-p)^(n-k) (lambda^n e^(-lambda))/n! \
&= (lambda e^(-lambda)) / k! sum_(n=k)^oo binom(n, k) p^k (1-p)^(n-k) lambda^n = (lambda e^(-lambda)) / k! sum_(n=k)^oo binom(n, k) (lambda p)^k (1-p)^(n-k) lambda^(n-k) \
&= (lambda p)^k e^(-lambda p)/k!
$
这种性质也可以理解成一种继承性质。

=== 母函数

设$X$为非负整数值随机变量，其分布为$P(X = k) = p_k, k = 0, 1, 2, ...$，则称
$
G(s) = E(s^X) = sum p_k s^k
$
为$X$的*概率母函数*，简称为*母函数*或者*生成函数*。相当于把$X=n$的概率放在多项式的$n$次项上。可以理解成一个Laplace变换。

==== 一些常见分布的母函数

- 二项分布$B(n, p)$的母函数为
  $
  G(s) = E(s^X) = sum_(k = 0)^n binom(n, k) p^k (1-p)^(n-k) s^k = (p s + 1 - p)^n
  $
- 泊松分布$P(lambda)$的母函数为
  $
  G(s) = E(s^X) = sum_(k = 0)^oo (lambda^k e^(-lambda))/k! s^k = e^(lambda(s-1))
  $
- 几何分布$G(p)$的母函数为
  $
  G(s) = E(s^X) = sum_(k = 0)^oo p (1-p)^k s^k = (p s) / (1 - (1-p)s)
  $

==== 母函数的性质

- $p_k = 1/k! G^(k)(0)$
- 设非负整数值随机变量$X_1, X_2, ... , X_n$*相互独立*(不一定同分布)，它们的母函数分别为$g_1, g_2, ... , g_n$，则$Y = sum_(i = 1)^n X_i$的母函数为
  $
  G(s) = product_(i = 1)^n g_i(s)
  $
  这就是Laplace变换乘法与卷积的关系。
母函数唯一的确定相应随机变量的概率分布律。从而可知概率分布律和母函数是一一对应的。

下面用一些基本的分布来说明母函数的性质。

考虑$X_k tilde B(1,p)$，其母函数为
$
G(s) = E(s^X) = p s + 1 - p
$
如果$X_i$互相独立，则$Y = sum_(i = 1)^n X_i$的母函数为
$
G(s) = (p s + 1 - p)^n
$
从而$Y ~ B(n,p)$。

考虑$X_k tilde B(n_k, p)$，其母函数为
$
G(s) = E(s^X) = (p s + 1 - p)^(n_k)
$
如果$X_i$互相独立，则$Y = sum_(i = 1)^n X_i$的母函数为
$
G(s) = product_(i = 1)^n (p s + 1 - p)^(n_i)
$
从而$Y ~ B(sum_(i = 1)^n n_i, p)$。

考虑$X_k tilde "Ge"(p)$，其母函数为
$
G(s) = E(s^X) = (p s )/ (1 - (1-p)s)
$
如果$X_i$互相独立，则$Y = sum_(i = 1)^n X_i$的母函数为
$
G(s) = product_(i = 1)^n (p s )/ (1 - (1-p)s) = (p^n s^n) / (1 - (1-p)s)^n
$
幂级数展开
$
G(s) &= p^n s^n (1- (1-p)s)^(-n) = p^n s^n sum_(k = 0)^oo binom(n+k-1, k) (1-p)^k s^k \
&= p^n (1-p)^n sum_(k = n)^oo binom(n-1, k-n) (1-p)^k s^k
$
从而
$
P(Y = k) = binom(n-1, k-1) p^n (1-p)^(k-n)
$
这是超几何分布的概率分布律。

可以证明Poisson分布的可加性：考虑$X_k tilde P(lambda_k)$，其母函数为
$
G(s) = E(s^X) = e^(lambda_k(s-1))
$
如果$X_i$互相独立，则$Y = sum_(i = 1)^n X_i$的母函数为
$
G(s) = product_(i = 1)^n e^(lambda_i(s-1)) = e^(sum_(i = 1)^n lambda_i(s-1))
$
从而$Y ~ P(sum_(i = 1)^n lambda_i)$。

- 母函数与期望和方差的关系
  $
  E(X) = G'(1)\
  D(X) = G''(1) + G'(1) - (G'(1))^2
  $

母函数的本质是期望，我们可以用全期望公式来求解母函数，进一步求解分布。

== 泊松过程

=== 泊松过程

若非负整数取值随机过程${N_t, t >= 0}$满足以下性质：
- $N_0 = 0$
- *增量独立性*：对于任意$0 <= t_1 < t_2 < ... < t_n$，$N_(t_2) - N_(t_1), N_(t_3) - N_(t_2), ... , N_(t_n) - N_(t_(n-1))$相互独立。#footnote[一般来说，总样本量很大的情况下才能满足]
- *增量平稳性*：对于任意$s, t >= 0$，$N_(t+s) - N_t$的分布与$N_s$的分布相同。
- *普通性*：对于任意$t>0$和充分小的$h>0$，有
  $
  P(N_(t+h) - N_t = 1) = lambda h + o(h)\
  P(N_(t+h) - N_t >= 2) = o(h)
  $
  其中$lambda$是常数，称为*强度*。
则称其为强度为$lambda$的*泊松过程*。

增量独立性是最重要的性质，在互不重叠的时间区间上信号发生数相互独立，这是后续导出常微分方程的关键。

增量平稳性意味着在$(s, s + t]$内信号发生数的概率只与时间间隔长短$t$有关，而与起点$s$无关。

在充分小的时间间隔$h$中，最多发生一个信号，发生两个（或以上）信号的概率是$h$的高阶无穷小，且信号发生的概率与$h$成正比。普通性的本质是局部线性化。

普通性蕴含了另一个关系，由概率的规范性，有
$
P(N_(t+h) - N_t = 0) = 1 - lambda h + o(h)
$
另外，普通性可以拓展：
$
P(N_(t+h) - N_t = k) = lambda_k + o(h)\
P(N_(t+h) - N_t >= k) = o(h)
$

这允许了泊松过程的泊松分布的推广。

=== 泊松过程的性质

如果$N_t$是强度为$lambda$的泊松过程，那么对于任意$s, t >= 0$，有
$
P(N_(t+s) - N_t = k) = (lambda s)^k e^(-lambda s) / k!
$
即$N_(t+s) - N_t ~ "Po"(lambda s)$。

_证明：_
令$p_k (t) = P(N_(t+s) - N_t = k)$，根据增量平稳性，有$p_k (t) = p_k (N_s)$。

先考虑$k = 0$的情况，有$p_0 (t + h)$



若非负整数值随机过程${N_t : t ≥ 0}$满足下面$3$条性质：

1. $N_0 = 0$；
2. 增量独立性；
3. $N_(s+t) - N_s tilde "Po"(λ t)$，则它是强度为$λ$的泊松过程。

该定理和上面性质结合起来，说明在条件1、2的情况下， 条件3和增量平稳性+普通性等价。*该定理也可以看成是泊松过程的另一定义。*

${N_t : t ≥ 0}$是强度为$λ$的泊松过程，则有
$
P(N_s = k | N_t = n) = binom(n, k) (s/t)^k (1-s/t)^(n-k)
$

该定理揭示了泊松过程和二项分布的关系。另外，它可以推广到多个时刻的条件概率情形，从而得到多项分布。

对于强度为$λ$泊松过程${N_t : t ≥ 0}$，我们有如下性质：
$
E(N_t) = λ t; D(N_t) = λ t; "Cov"(N_s, N_t) = λ(s and t)
$

#newpara()

*泊松过程的相加不变*

令$X_t$和$Y_t$为泊松过程$X = {X_t : t ≥ 0}$和$Y = {Y_t : t ≥ 0}$的强度分别$λ_1$和$λ_2$。$N_t = X_t + Y_t$，由于$X$和$Y$相互独立，容易验证$N_t$满足上面定理的第（1）、（2）条。再根据定理2.1，可知
$
N_(s+t) - N_s tilde "Po"((λ_1 + λ_2)t)
$
因此$N = {N_t : t ≥ 0}$是强度为$λ = λ_1 + λ_2$的泊松过程。

*泊松过程的选择不变*

$N_t$是强度为$λ$的泊松过程，

=== Poisson过程的推广

==== 非时齐泊松过程

若非负整数值随机过程${N_t : t ≥ 0}$满足下面3条性质，则称其为强度函数为$λ(t)$的非时齐泊松过程：

==== 复合泊松过程

==== 更新过程

= 连续型随机变量

== 连续型随机变量

=== 概率分布函数及概率密度函数

设X是一个随机变量，函数
$
F(x) = P(X ≤ x), x in RR
$
称为$X$的*概率分布函数*（简称分布函数）。

设随机变量$X$的分布函数为$F(x)$，如果存在非负可积函数$f(x)$，使得
$
F(x) = integral_(-oo)^x f(s) dd(s)
$
则称$X$为*连续型随机变量*，称$f(x)$为$X$的概率密度函数（简称概率密度）。

考虑离散型随机变量$X$，其概率分布律为
$
p_k = P(X = a_k), k = 1, 2, ...
$
其中$a_1 < a_2 < ... < a_n$，则$X$的分布函数为
$
F(x) = sum_(a_k <= x) p_k 
$
因此$F(x)$是分段常值函数，$x = a_1,a_2, ...,a_n$是$F(x)$的跳跃点，其跳跃的高度为$p_1, p_2, ..., p_n$。

随机变量$X$的分布函数$F(x)$有如下性质：
- $0 ≤ F(x) ≤ 1, x in RR$
- $F(x)$是非减函数：如$x_1 < x_2$，则$F(x_1) ≤ F(x_2)$
- $lim_(-oo) F(x) = 0$ ，$lim_(+oo) F(x) = 1$
- $F(x)$右连续，即$lim_(x→x_0+) F(x) = F(x_0)$
- $P(X = x) = F(x) - F(x^-)$，其中$F(x^-) = lim_(x→x_0-) F(x)$

连续型随机变量X的概率密度f(x)有如下性质：
- $f(x) ≥ 0, x in RR$
- $integral_(-oo)^oo f(x) dd(x) = 1$
- $P(a ≤ X ≤ b) = integral_a^b f(x) dd(x)$
- 若$f(x)$在$x_0$处连续，则$F'(x_0) = f(x_0)$

=== 数学期望和方差

设连续型随机变量$X$的概率密度为$f(x)$，若积分
$
E(X) = integral_(-oo)^oo x f(x) dd(x)
$
绝对收敛，则称$E(X)$为$X$的*数学期望*。

设连续型随机变量$X$的概率密度为$f(x)$，函数$phi : RR -> RR$连续，则$Y = φ(X)$也是一个随机变量，若积分
$
E(Y) = integral_(-oo)^oo φ(x) f(x) dd(x)
$
绝对收敛，则称$E(Y)$为$Y$的*数学期望*。

设连续型随机变量X的概率密度为$f(x)$，若$E ((X - E(X))^2)$存在，则称为$X$的方差，即
$
D(X) = E((X - E(X))^2) = integral_(-oo)^oo (x - E(X))^2 f(x) dd(x)
$
称$sqrt(D(X))$为X的标准差。

考虑随机变量$X,Y$，若期望$E(X),E(Y)$均存在，则有
- $|E(X)| <= E(|X|)$
- 对实数$a < b$，若$a <= X <= b$，则$a <= E(X) <= b$
- 若$X <= Y$，则$E(X) <= E(Y)$
- 对实数$a, b$，有$E(a X + b Y) = a E(X) + b E(Y)$
- 若$X,Y$相互独立，则$E(X Y) = E(X) E(Y)$
- 【Schwarz不等式】$|E(X Y)|^2 <= E(X^2) E(Y^2)$

若方差$D(X),D(Y)$均存在，则有
- 对实数$a, b$，有$D(a X + b) = a^2 D(X)$
- 若$X,Y$相互独立，则$D(X + Y) = D(X) + D(Y)$
- $D(X) = 0$当且仅当$P(X = E(X)) = 1$，即$X$几乎必然是一个常数

也有Chebyshev不等式：
$
P(|X - E(X)| >= epsilon) <= D(X) / epsilon^2
$
_证明：_
$
P(|X - E(X)| >= epsilon) &= integral_(|X - E(X)| >= epsilon) f(x) dd(x) \
&<= integral_(|X - E(X)| >= epsilon) (x - E(X))^2 / epsilon^2 f(x) dd(x) \
&= 1/epsilon^2 integral_(|X - E(X)| >= epsilon) (x - E(X))^2 f(x) dd(x) \
&= <= 1/epsilon^2 integral_(-oo)^oo (x - E(X))^2 f(x) dd(x) = D(X) / epsilon^2
$
若取$epsilon = k sqrt(D(X))$，则有
$
P(|X - E(X)| >= k sqrt(D(X))) <= 1/k^2
$
可以让我们从另一个角度观察随机变量值远离其数学期望的概率。

考虑强度为$λ$的泊松过程${N_t : t ≥ 0}$。相应的，我们可以导出另一组随机过程${X_n : n = 1, 2...}$，它给出泊松过程中第$n$个信号的发生时间。求$E(X_n)$和$D(X_n)$。 

对$t > 0$，先求$X_n$对应的分布函数$F_n (t)$和概率密度$f_n (t)$。由于$X_n$是第$n$个信号的发生时间，有
$
F_n (t) = P(X_n <= t) = P(N_t >= n) = 1 - P(N_t < n) = 1 - sum_(k = 0)^(n-1) e^(-λ t) (λ t)^k / k!
$
对应的概率密度为
$
f_n (t) = F_n'(t) = λ e^(-λ t) (λ t)^(n-1) / (n-1)!
$
对$t<=0$时，$F_n (t) = 0$，$f_n (t) = 0$。从而得到
$
E(X_n) = integral_0^oo t f_n (t) dd(t) = n/λ\
E(X_n^2) = integral_0^oo t^2 f_n (t) dd(t) = n(n+1)/λ^2\
D(X_n) = E(X_n^2) - (E(X_n))^2 = n/λ^2
$

== 常见的连续型随机变量

=== 均匀分布

如果连续型随机变量$X$的概率密度为：
$
f(x) = 1/(b-a) I_[a,b]
$
则称$X$服从$(a,b)$上的均匀分布，记作$X ~ U(a,b)$。

均匀分布的数学期望和方差为
$
E(X) = (a+b)/2\
D(X) = (b-a)^2 / 12
$

=== 指数分布

如果连续型随机变量$X$的概率密度为：
$
f(x) = λ e^(-λ x) I_[0, +oo)
$
则称$X$服从参数为$λ$的指数分布，记作$X ~ "Exp"(λ)$。

指数分布的数学期望和方差为
$
E(X) = 1/λ\
D(X) = 1/λ^2
$
和Poisson过程的联系：指数分布是泊松过程的等待时间分布。

*指数分布的无记忆性*：若$X ~ "Exp"(λ)$，则对任意$s, t > 0$，有
$
P(X > s + t | X > s) = P(X > t)
$
_证明：_
$
P(X > s + t | X > s) &= P(X > s + t, X > s) / P(X > s) \
&= P(X > s + t) / P(X > s) = e^(-λ(s+t)) / e^(-λ s) = e^(-λ t) = P(X > t)
$

#newpara()

*指数分布的无老化性*：$X$是取值非负的随机变量，则$X ~ "Exp"(λ)$当且仅当对任意$t > 0$和充分小的$h > 0$，有
$
P(t < X < t + h | X > t) = λ h + o(h)
$
_证明：_
先证明必要性
$
P(t < X < t + h | X > t) &= P(t < X < t + h) / P(X > t) = (e^(-λ t) - e^(-λ (t+h))) / e^(-λ t) = λ h + o(h)
$
再证明充分性
$
(F(x + h) - F(x)) / (1 - F(x)) = λ h + o(h)\
(F(x + h) - F(x)) / h = λ (1 - F(x)) + o(1 - F(x))\
$
令$h -> 0$，有
$
f(x) = λ e^(-λ x)
$
从而$X ~ "Exp"(λ)$。

=== 正态分布

如果连续型随机变量$X$的概率密度为：
$
f(x) = 1/(sqrt(2π) σ) e^(-(x-μ)^2 / 2σ^2)
$
则称$X$服从参数为$μ, σ^2$的*正态分布*，记作$X ~ N(μ, σ^2)$。特别地，若$μ = 0, σ = 1$，则称$X$服从*标准正态分布*，记作$X ~ N(0, 1)$。

下面证明$integral_(-oo)^oo e^(-x^2) dd(x) = sqrt(π)$。
$
integral_(-oo)^oo e^(-x^2) dd(x) &= integral_(-oo)^oo u^(-1/2) e^(-u) dd(u) = Gamma(1/2) = sqrt(π)
$

#newpara()

正态分布的数学期望和方差为
$
E(X) = μ\
E(X^2) = μ^2 + σ^2\
D(X) = σ^2
$

若$X$服从标准正态分布，习惯上人们用$φ(x)$和$Φ(x)$分别表示概率密度和分布函数，即
$
φ(x) = 1/√(2π) e^(-x^2 / 2)\
Φ(x) = integral_(-oo)^x φ(t) dd(t)
$
$φ(x)$和$Φ(x)$具有下面性质：
- $φ(-x) = φ(x)$
- $x =0$时，$φ(x) = 1/√(2π)$取到最大值
- $Φ(-x) = 1 - Φ(x)$
- 若$X ~ N(μ, σ^2)$，则$F(x) = Φ((x-μ)/σ)$。任何正态分布都可以通过一个线性变化转化为标准正态分布，此过程也称为对随机变量$X$进行标准化。

#grid(columns: (1fr,1fr),
  [#figure(
    image("pic/2024-05-07-11-55-47.png", width: 80%),
    caption: [
      正态分布的概率密度函数
    ],
  )],
  [#figure(
    image("pic/2024-05-07-11-56-05.png", width: 80%),
    caption: [
      正态分布的分布函数
    ]
  )]
)

概率密度$f(x)$有下面性质：
- 关于$x = µ$对称；
- 当$x = µ$时， $f(x)$取最大值 $1/(√(2π) σ)$；
- $f(x)$在$x = µ ± σ$处有拐点；
- $σ$越大，$f(x)$的图像越“胖”，取值越分散，$σ$越小，$f(x)$的图像越“瘦”，取值越集中；
- $f(x)$存在任意阶连续导数。

== 二维连续型随机变量

=== 二维连续型随机变量

设随机试验的样本空间为$Ω$，$X$和$Y$是定义在$Ω$上的两个随机变量，我们称向量$(X, Y)$为二维随机变量或二维随机向量。

设$(X, Y)$的联合概率密度为$f(x, y)$，若对任意$D ⊆ RR^2$，有
$
P((X, Y) ∈ D) = integral.double_D f(x, y) dd(x) dd(y)
$

= 布朗运动和极限定理

== 特征函数和高斯分布

